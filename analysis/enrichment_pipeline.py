#!/usr/bin/env python3
â€œâ€â€
Streamlined 5-Feature NLP Enrichment Pipeline for Podcast Segments
Features: Sentence-BERT, Granular Emotions, Persuasive Techniques, Basic Sentiment, Topic Modeling
â€œâ€â€

import json
import os
import glob
from pathlib import Path
from datetime import datetime
import argparse
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm
import re

# Persuasive technique keywords (simple but effective)

PERSUASIVE_TECHNIQUES = {
â€˜loaded_languageâ€™: [
â€˜radicalâ€™, â€˜extremeâ€™, â€˜dangerousâ€™, â€˜corruptâ€™, â€˜eliteâ€™, â€˜establishmentâ€™,
â€˜mainstream mediaâ€™, â€˜fake newsâ€™, â€˜propagandaâ€™, â€˜brainwashedâ€™, â€˜sheepâ€™,
â€˜deep stateâ€™, â€˜globalistâ€™, â€˜agendaâ€™, â€˜narrativeâ€™, â€˜indoctrinationâ€™
],
â€˜fear_appealâ€™: [
â€˜threatâ€™, â€˜dangerâ€™, â€˜crisisâ€™, â€˜emergencyâ€™, â€˜catastropheâ€™, â€˜disasterâ€™,
â€˜collapseâ€™, â€˜destroyâ€™, â€˜attackâ€™, â€˜invasionâ€™, â€˜warâ€™, â€˜violenceâ€™,
â€˜afraidâ€™, â€˜scaredâ€™, â€˜terrifyingâ€™, â€˜nightmareâ€™
],
â€˜whataboutismâ€™: [
â€˜what aboutâ€™, â€˜but what aboutâ€™, â€˜meanwhileâ€™, â€˜whileâ€™, â€˜instead ofâ€™,
â€˜rather thanâ€™, â€˜compared toâ€™, â€˜unlikeâ€™, â€˜whereasâ€™
],
â€˜causal_oversimplificationâ€™: [
â€˜because ofâ€™, â€˜thanks toâ€™, â€˜due toâ€™, â€˜caused byâ€™, â€˜result ofâ€™,
â€˜blameâ€™, â€˜faultâ€™, â€˜responsible forâ€™, â€˜leads toâ€™, â€˜createsâ€™
]
}

# Topic categories for keyword-based detection

PREDEFINED_TOPICS = {
â€˜politicsâ€™: [â€˜electionâ€™, â€˜presidentâ€™, â€˜governmentâ€™, â€˜policyâ€™, â€˜democratâ€™, â€˜republicanâ€™,
â€˜congressâ€™, â€˜senateâ€™, â€˜voteâ€™, â€˜campaignâ€™, â€˜politicianâ€™, â€˜administrationâ€™,
â€˜bidenâ€™, â€˜trumpâ€™, â€˜harrisâ€™, â€˜politicalâ€™, â€˜legislationâ€™, â€˜white houseâ€™],
â€˜conspiracyâ€™: [â€˜deep stateâ€™, â€˜conspiracyâ€™, â€˜cover upâ€™, â€˜eliteâ€™, â€˜agendaâ€™, â€˜narrativeâ€™,
â€˜mainstream mediaâ€™, â€˜globalistâ€™, â€˜shadowâ€™, â€˜secretâ€™, â€˜hiddenâ€™, â€˜truthâ€™,
â€˜psyopâ€™, â€˜false flagâ€™, â€˜controlledâ€™, â€˜puppetâ€™, â€˜awakeningâ€™, â€˜red pillâ€™],
â€˜healthâ€™: [â€˜vaccineâ€™, â€˜covidâ€™, â€˜healthâ€™, â€˜medicalâ€™, â€˜doctorâ€™, â€˜treatmentâ€™, â€˜pandemicâ€™,
â€˜virusâ€™, â€˜immunityâ€™, â€˜pharmaceuticalâ€™, â€˜medicineâ€™, â€˜hospitalâ€™, â€˜pfizerâ€™,
â€˜modernaâ€™, â€˜mrnaâ€™, â€˜side effectsâ€™, â€˜natural immunityâ€™, â€˜ivermectinâ€™],
â€˜economyâ€™: [â€˜economyâ€™, â€˜inflationâ€™, â€˜marketâ€™, â€˜moneyâ€™, â€˜dollarâ€™, â€˜bankâ€™, â€˜financialâ€™,
â€˜recessionâ€™, â€˜jobsâ€™, â€˜unemploymentâ€™, â€˜stockâ€™, â€˜cryptoâ€™, â€˜currencyâ€™,
â€˜federal reserveâ€™, â€˜interest ratesâ€™, â€˜housingâ€™, â€˜debtâ€™, â€˜collapseâ€™],
â€˜technologyâ€™: [â€˜aiâ€™, â€˜techâ€™, â€˜social mediaâ€™, â€˜algorithmâ€™, â€˜dataâ€™, â€˜privacyâ€™, â€˜internetâ€™,
â€˜digitalâ€™, â€˜surveillanceâ€™, â€˜platformâ€™, â€˜silicon valleyâ€™, â€˜censorshipâ€™,
â€˜twitterâ€™, â€˜facebookâ€™, â€˜googleâ€™, â€˜tiktokâ€™, â€˜artificial intelligenceâ€™],
â€˜conflictâ€™: [â€˜warâ€™, â€˜militaryâ€™, â€˜ukraineâ€™, â€˜russiaâ€™, â€˜chinaâ€™, â€˜nuclearâ€™, â€˜weaponâ€™,
â€˜defenseâ€™, â€˜natoâ€™, â€˜conflictâ€™, â€˜invasionâ€™, â€˜soldierâ€™, â€˜pentagonâ€™,
â€˜droneâ€™, â€˜missileâ€™, â€˜borderâ€™, â€˜israelâ€™, â€˜gazaâ€™, â€˜middle eastâ€™],
â€˜cultureâ€™: [â€˜wokeâ€™, â€˜cultureâ€™, â€˜genderâ€™, â€˜transâ€™, â€˜identityâ€™, â€˜cancelâ€™, â€˜diversityâ€™,
â€˜traditionalâ€™, â€˜valuesâ€™, â€˜societyâ€™, â€˜educationâ€™, â€˜childrenâ€™, â€˜pronounâ€™,
â€˜critical raceâ€™, â€˜groomingâ€™, â€˜indoctrinationâ€™, â€˜familyâ€™, â€˜christianâ€™],
â€˜mediaâ€™: [â€˜mediaâ€™, â€˜newsâ€™, â€˜journalistâ€™, â€˜reportâ€™, â€˜propagandaâ€™, â€˜narrativeâ€™, â€˜storyâ€™,
â€˜coverageâ€™, â€˜biasâ€™, â€˜fact checkâ€™, â€˜misinformationâ€™, â€˜disinformationâ€™,
â€˜alternative mediaâ€™, â€˜independentâ€™, â€˜mainstreamâ€™, â€˜legacy mediaâ€™]
}

def load_models():
â€œâ€â€œLoad all required models once at startup.â€â€â€
print(â€œğŸ”„ Loading modelsâ€¦â€)

```
models = {
    'sentence_bert': SentenceTransformer('all-mpnet-base-v2'),
    'emotion': pipeline('text-classification', 
                      'j-hartmann/emotion-english-distilroberta-base',
                      return_all_scores=True),
    'sentiment': pipeline('sentiment-analysis', 
                        'distilbert-base-uncased-finetuned-sst-2-english')
}

print("âœ… All models loaded successfully!")
return models
```

def detect_persuasive_techniques(text):
â€œâ€â€œDetect persuasive techniques using keyword matching.â€â€â€
text_lower = text.lower()
detected = []

```
for technique, keywords in PERSUASIVE_TECHNIQUES.items():
    for keyword in keywords:
        if keyword in text_lower:
            detected.append(technique)
            break  # Only count each technique once per segment

return detected
```

def detect_topics_keywords(text):
â€œâ€â€œKeyword-based topic detection with confidence scores.â€â€â€
text_lower = text.lower()
detected_topics = []
topic_scores = {}

```
for topic, keywords in PREDEFINED_TOPICS.items():
    matches = sum(1 for keyword in keywords if keyword in text_lower)
    if matches > 0:
        # Score based on number of keyword matches normalized by total keywords
        score = matches / len(keywords)
        topic_scores[topic] = round(score, 3)
        detected_topics.append(topic)

return detected_topics, topic_scores
```

def extract_key_phrases(text, max_features=5):
â€œâ€â€œExtract key phrases using TF-IDF.â€â€â€
try:
# Use TF-IDF to find important n-grams
vectorizer = TfidfVectorizer(
max_features=max_features,
ngram_range=(1, 3),  # Unigrams, bigrams, and trigrams
stop_words=â€˜englishâ€™,
min_df=1
)

```
    # Fit and transform the single document
    tfidf_matrix = vectorizer.fit_transform([text])
    feature_names = vectorizer.get_feature_names_out()
    
    # Get the TF-IDF scores
    scores = tfidf_matrix.toarray()[0]
    
    # Create phrase-score pairs and sort by score
    phrase_scores = [(feature_names[i], scores[i]) 
                    for i in range(len(scores)) if scores[i] > 0]
    phrase_scores.sort(key=lambda x: x[1], reverse=True)
    
    # Return top phrases
    return [phrase for phrase, score in phrase_scores[:max_features]]
except Exception as e:
    return []
```

def get_emotion_scores(emotion_results):
â€œâ€â€œConvert emotion pipeline results to clean scores.â€â€â€
emotion_scores = {}

```
for result in emotion_results[0]:  # First (and only) result
    emotion_scores[result['label']] = round(result['score'], 3)

return emotion_scores
```

def enrich_segment(segment, models):
â€œâ€â€œEnrich a single segment with all 5 features.â€â€â€
text = segment.get(â€˜textâ€™, â€˜â€™)

```
# Skip very short segments
if len(text.strip()) < 10:
    return segment

try:
    # 1. Sentence-BERT Embeddings
    embedding = models['sentence_bert'].encode(text)
    segment['embedding'] = embedding.tolist()  # Convert numpy to list for JSON
    
    # 2. Granular Emotion Classification
    emotion_results = models['emotion'](text)
    emotion_scores = get_emotion_scores(emotion_results)
    segment['emotion_scores'] = emotion_scores
    
    # Get dominant emotion
    dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])
    segment['dominant_emotion'] = {
        'label': dominant_emotion[0],
        'score': dominant_emotion[1]
    }
    
    # 3. Basic Sentiment
    sentiment_result = models['sentiment'](text)[0]
    segment['sentiment'] = {
        'label': sentiment_result['label'],
        'score': round(sentiment_result['score'], 3)
    }
    
    # 4. Persuasive Techniques
    techniques = detect_persuasive_techniques(text)
    segment['persuasive_techniques'] = techniques
    
    # 5. Topic Modeling
    topics, topic_scores = detect_topics_keywords(text)
    segment['detected_topics'] = topics
    segment['topic_scores'] = topic_scores
    
    # Key phrase extraction
    key_phrases = extract_key_phrases(text)
    segment['key_phrases'] = key_phrases
    
    # Primary topic (highest scoring)
    if topic_scores:
        primary_topic = max(topic_scores.items(), key=lambda x: x[1])
        segment['primary_topic'] = {
            'topic': primary_topic[0],
            'confidence': primary_topic[1]
        }
    else:
        segment['primary_topic'] = None
    
    # Add processing metadata
    segment['enriched_at'] = datetime.now().isoformat()
    segment['enrichment_version'] = '1.1_with_topics'
    
except Exception as e:
    print(f"âš ï¸ Error enriching segment: {e}")
    # Add empty enrichment fields to maintain consistency
    segment['embedding'] = None
    segment['emotion_scores'] = {}
    segment['dominant_emotion'] = {'label': 'unknown', 'score': 0.0}
    segment['sentiment'] = {'label': 'UNKNOWN', 'score': 0.0}
    segment['persuasive_techniques'] = []
    segment['detected_topics'] = []
    segment['topic_scores'] = {}
    segment['key_phrases'] = []
    segment['primary_topic'] = None

return segment
```

def aggregate_episode_topics(segments):
â€œâ€â€œAggregate topics across all segments in an episode.â€â€â€
all_topics = {}
total_segments = len(segments)

```
for segment in segments:
    for topic in segment.get('detected_topics', []):
        all_topics[topic] = all_topics.get(topic, 0) + 1

# Calculate topic percentages
topic_distribution = {
    topic: {
        'count': count,
        'percentage': round(count / total_segments * 100, 2)
    }
    for topic, count in all_topics.items()
}

# Sort by count
sorted_topics = sorted(topic_distribution.items(), 
                      key=lambda x: x[1]['count'], 
                      reverse=True)

return dict(sorted_topics)
```

def process_file(file_path, models, output_dir):
â€œâ€â€œProcess a single JSON file.â€â€â€
try:
with open(file_path, â€˜râ€™, encoding=â€˜utf-8â€™) as f:
data = json.load(f)
except Exception as e:
print(fâ€âŒ Error reading {file_path}: {e}â€)
return 0, 0

```
segments = data.get('segments', [])
if not segments:
    print(f"âš ï¸ No segments in {file_path}")
    return 0, 0

processed = 0
skipped = 0

# Process segments with progress bar
for segment in tqdm(segments, desc=f"Processing {os.path.basename(file_path)}", leave=False):
    try:
        enrich_segment(segment, models)
        processed += 1
    except Exception as e:
        print(f"âš ï¸ Skipping segment: {e}")
        skipped += 1

# Add episode-level topic summary
data['episode_topic_summary'] = aggregate_episode_topics(segments)

# Save enriched file
output_file = output_dir / f"{Path(file_path).stem}_enriched.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(data, f, indent=2, ensure_ascii=False)

return processed, skipped
```

def main():
parser = argparse.ArgumentParser(description=â€˜Streamlined NLP enrichment for podcast segmentsâ€™)
parser.add_argument(â€˜input_dirâ€™, help=â€˜Directory containing processed transcript filesâ€™)
parser.add_argument(â€™â€“outputâ€™, â€˜-oâ€™, required=True, help=â€˜Output directory for enriched filesâ€™)
parser.add_argument(â€™â€“batch-sizeâ€™, type=int, default=50, help=â€˜Number of files to process in batchâ€™)

```
args = parser.parse_args()

input_dir = Path(args.input_dir)
output_dir = Path(args.output)
output_dir.mkdir(exist_ok=True)

# Find all JSON files
json_files = list(input_dir.glob('*_with_ids.json'))

if not json_files:
    print(f"âŒ No JSON files found in {input_dir}")
    return

print(f"ğŸ“‚ Found {len(json_files)} files to enrich")
print(f"ğŸ’¾ Output directory: {output_dir}")

# Load models once
models = load_models()

total_processed = 0
total_skipped = 0

# Process files
for file_path in tqdm(json_files, desc="Processing files"):
    processed, skipped = process_file(file_path, models, output_dir)
    total_processed += processed
    total_skipped += skipped
    
    # Log progress every 100 files
    if (total_processed + total_skipped) % 1000 == 0:
        print(f"ğŸ“Š Progress: {total_processed:,} segments enriched, {total_skipped:,} skipped")

print("=" * 60)
print(f"ğŸ‰ Enrichment complete!")
print(f"âœ… Total segments enriched: {total_processed:,}")
print(f"âš ï¸ Total segments skipped: {total_skipped:,}")
print(f"ğŸ“ Enriched files saved to: {output_dir}")
print(f"ğŸ·ï¸ Features added: embeddings, emotions, sentiment, persuasive techniques, topics")
```

if **name** == â€œ**main**â€:
main()

